{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_with_RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSDSw4jsps2gVJOVLKaVun",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee00206/Tensorflow_for_beginners/blob/main/NLP_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sUNp22PwNpR"
      },
      "source": [
        "# **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z95CG6A5wP6q"
      },
      "source": [
        "## **Movie Review Dataset**\n",
        "For the analysis, the IMDB movie review dataset from keras will be loaded. This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and has a label as either positive or negative. Each review is encoded by integers that represents how common a word is in the entire dataset. For example a word encoded by the integer 3 means that it is the 3rd most common word in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgiKliXP1E7_"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeFgEi6qwzxf",
        "outputId": "dba81948-4fbf-426a-d3db-cc9537deaf7d"
      },
      "source": [
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxn30v6ExFoB",
        "outputId": "2bbbb432-9fde-42ec-feca-8f3705613b19"
      },
      "source": [
        "# Look at one review\n",
        "train_data[0]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqT_adgUxbLP"
      },
      "source": [
        "## **More Preprocessing**\n",
        "Since different length data cannot be passed into the neural network, the following procedure will be followed to make each review the same length:\n",
        "* If the review is greater than 250 words then trim off the extra words\n",
        "* If the review is less than 250 words add teh necessay amount of's to make it equal to 250"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnE6hFZ5x7rK",
        "outputId": "0d6816a7-14cb-457d-c107-5f15b21a349d"
      },
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)\n",
        "\n",
        "# Example\n",
        "len(train_data[1])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyYq011XyFFO"
      },
      "source": [
        "## **Creating the Model**\n",
        "A word embedding layer will be used as the first layer in the model and a LSTM layer will be added afterwards that feeds into a dense node to get the predicted sentiment.<br>\n",
        "32 stands for the output dimension of the vectors generated by the embedding layer. The value can be changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L23iR2Wqysod"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "                             tf.keras.layers.LSTM(32),\n",
        "                             tf.keras.layers.Dense(1, activation = \"sigmoid\")   # Dense of 1: if the number is greater than 0.5, the review is considered as positive and vice versa for the negative review.\n",
        "])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5eseW1LzAxX",
        "outputId": "1bb1f2d4-cbde-4890-cbc8-8b7914e39861"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi8neC7LznE2"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dESJBLxfzo9w",
        "outputId": "7ccc161f-01fe-420e-dbc4-33dc447f873c"
      },
      "source": [
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"rmsprop\", metrics = ['acc'])\n",
        "history = model.fit(train_data, train_labels, epochs = 10, validation_split = 0.2)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 17s 24ms/step - loss: 0.5397 - acc: 0.7228 - val_loss: 0.2894 - val_acc: 0.8874\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.2354 - acc: 0.9137 - val_loss: 0.2775 - val_acc: 0.8884\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.1709 - acc: 0.9383 - val_loss: 0.2652 - val_acc: 0.8938\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.1370 - acc: 0.9495 - val_loss: 0.2704 - val_acc: 0.8890\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.1209 - acc: 0.9578 - val_loss: 0.3486 - val_acc: 0.8902\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.1078 - acc: 0.9637 - val_loss: 0.3021 - val_acc: 0.8940\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.0887 - acc: 0.9708 - val_loss: 0.3487 - val_acc: 0.8770\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.0790 - acc: 0.9738 - val_loss: 0.3136 - val_acc: 0.8874\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.0641 - acc: 0.9790 - val_loss: 0.3701 - val_acc: 0.8796\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.0576 - acc: 0.9812 - val_loss: 0.3886 - val_acc: 0.8818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8THi_Td0bo_",
        "outputId": "0cf69c82-1669-4d6b-e60b-9c0bafd6d66f"
      },
      "source": [
        "# Evaluate the model\n",
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 6s 7ms/step - loss: 0.4680 - acc: 0.8563\n",
            "[0.4680071771144867, 0.8563200235366821]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2fHIFPS0mNt"
      },
      "source": [
        "## **Making Predictions**\n",
        "Since the reviews are encoded well need to convert any review that we write into that form so the network can understand it. To do that well load the encodings from the dataset and use them to encode our own data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR1rc0sj1Jpw",
        "outputId": "55bf3ad6-fe86-4dd7-e7ab-cebd7f95465d"
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)   # convert text into called tokens\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48099IaW4Bqr",
        "outputId": "6a2d16e9-54c8-4dbc-a528-d113456a99c8"
      },
      "source": [
        "# make a decode function\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "  PAD = 0\n",
        "  text = \"\"\n",
        "  for num in integers:\n",
        "    if num != PAD:\n",
        "      text += reverse_word_index[num] + \" \"\n",
        "  return text[:-1]  # return the text except the last space(\" \")\n",
        "\n",
        "print(decode_integers(encoded))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that movie was just amazing so amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbEhWriJ44xV",
        "outputId": "99f8874c-52c9-4e19-a976-2ef57a84e3bb"
      },
      "source": [
        "# make a prediction\n",
        "def predict_review(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1, 250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred)\n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great\"\n",
        "predict_review(positive_review)\n",
        "\n",
        "negative_review = \"That movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict_review(negative_review)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.80868465]\n",
            "[0.39967075]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBqpI9mU9z6U"
      },
      "source": [
        "# **RNN Play Generator**\n",
        "For this section, RNN will be used to generate a play. The RNN will be an example of something to recreate and it will learn how to write a version of it on its own. This will be done by using a character predictive model that will take as input a variable length sequence and predict the next character. The model can be used many times in a row with the output from the last prediction as the input for the next call to generate a sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMiVRDrTA4Xm"
      },
      "source": [
        "## **Dataset**\n",
        "The data will be an extact from a shakesphere play."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMm2fFQBBhag"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GpjmtYIBpYk"
      },
      "source": [
        "## **Read Contents of File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdpy6Z_uCBlt",
        "outputId": "66a12623-4c4e-48f5-c0db-d37ac27b2ed3"
      },
      "source": [
        "# read, then decode for py2 compat\n",
        "text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Ljj5RpCRl4",
        "outputId": "6f733241-8aae-4bd9-d3ee-b34f239275d0"
      },
      "source": [
        "# the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eq6CNEnCYFJ"
      },
      "source": [
        "## **Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agp1BLlvCeVI"
      },
      "source": [
        "vocab = sorted((set(text))) # sort all the unique characters in text\n",
        "\n",
        "# creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}   # i: starting from 0, u: string in vocab -> pair (0(i), string(u)) and so on\n",
        "idx2char = np.array(vocab)   # returns the initial vocabulary(vocab) in a list of array\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd0EBvzKCwRg",
        "outputId": "e2e6dcb1-61b1-4d21-e3ed-12c93026e363"
      },
      "source": [
        "print(\"Text: \", text[:13])\n",
        "print(\"Encoded: \", text_to_int(text[:13]))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:  First Citizen\n",
            "Encoded:  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zdKYwZaE-tn",
        "outputId": "839c4c70-463e-4abb-cd73-ff827440b7e8"
      },
      "source": [
        "# make a function that can convert the numeric values to text\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta3gu2TQE-YR"
      },
      "source": [
        "## **Creating Training Examples**\n",
        "Remember the task is to feed the model a sequence and have it return the next character. This means it is necessary to split the text data from above into many shorter sequences that can be passed to the model as training examples.<br>\n",
        "The training examples are going to be a *seq_length* sequence as input and a *seq_length* sequence as the output where that sequence is the original sequence shifted one letter to the right. For example:<br>\n",
        "input: Hell<br>\n",
        "output: ello<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDSAokCiGPHU"
      },
      "source": [
        "# create a stream of characters from the text data\n",
        "\n",
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text) // (seq_length + 1)  # to return the next character, we need 101 characters (seq_length + 1) as a training example\n",
        "\n",
        "# create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)  # convert the entire stream dataset into characters"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQHeTOpTG2fM"
      },
      "source": [
        "# use the batch method to turn this stream of characters into batches of desired length\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IhP7u7rIyT-"
      },
      "source": [
        "# uses the sequences of length 101 and split them into input and output\n",
        "def split_input_target(chunk):  # for the example: hello\n",
        "  input_text = chunk[:-1]   # hell\n",
        "  target_text = chunk[1:]   # ello\n",
        "  return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)   # use map to apply the above function to every entry"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Buh_BUM-JPzy",
        "outputId": "77e6bbf0-fa4a-44bd-b918-0658b451cc13"
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print('INPUT\\n')\n",
        "  print(int_to_text(x))\n",
        "  print('\\nOUTPUT\\n')\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw-rHO0oJ-e9"
      },
      "source": [
        "# make training batches\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)   # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences, so it doesn't attempt to shuffle the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements)\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNH80Fhakapg"
      },
      "source": [
        "## **Building the Model**\n",
        "An embedding layer a LSTM and one dense layer that contains a node for each unique character in the training data will be used. The dense layer will give a probability distribution over all nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2_-6zmfjp4A",
        "outputId": "2dee6d82-3b39-4f4b-8394-11c879e0b192"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                         batch_input_shape = [batch_size, None]),   # None: we don't know how long the sequence is\n",
        "                              tf.keras.layers.LSTM(rnn_units,\n",
        "                                                   return_sequences = True,\n",
        "                                                   stateful = True,\n",
        "                                                   recurrent_initializer = 'glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z4DynCLmtOd"
      },
      "source": [
        "## **Creating a Loss Function**\n",
        "It is necessary to create loss function for this problem, because the model will output a (64, sequence_length, 65) shaped tensor that represents the probability distribution of each character at each timestep for every sequence in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtmAtd8MnKHT",
        "outputId": "b8106df7-c90c-4fe2-bc60-3cedb86c3b90"
      },
      "source": [
        "# Look at a sample input and the output from the untrained model to understand what the model actually returns\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask the model for a prediction on the first batch of training data\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")   # print out the output shape"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxVatkjvnjfg",
        "outputId": "3a2a0ba4-ad02-478d-f7da-9ae7f24f1874"
      },
      "source": [
        "# the prediction is an array of 64 arrays, one for each enbry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 3.3866814e-03  5.2238037e-03 -5.6300871e-03 ... -3.1583738e-03\n",
            "    1.9084960e-03  2.7738381e-03]\n",
            "  [ 4.5772679e-03  1.2467699e-03 -3.4201383e-03 ... -3.9539426e-03\n",
            "    2.9788343e-03  2.6930752e-03]\n",
            "  [ 2.7872645e-03 -4.4394698e-04 -5.4929880e-03 ... -2.2290377e-03\n",
            "   -3.9155674e-03  7.4616997e-03]\n",
            "  ...\n",
            "  [ 5.4734237e-03 -8.7275412e-03 -2.2610102e-04 ... -1.2706110e-03\n",
            "   -4.0140245e-03  2.7517264e-03]\n",
            "  [ 3.7500416e-03 -6.1370987e-03  1.0347291e-03 ... -3.7090508e-03\n",
            "   -1.0794081e-02  5.4155961e-03]\n",
            "  [ 6.3601974e-03 -5.0857645e-03  2.4561374e-03 ... -3.0768914e-03\n",
            "   -5.5794092e-03  3.3621816e-03]]\n",
            "\n",
            " [[ 3.3866814e-03  5.2238037e-03 -5.6300871e-03 ... -3.1583738e-03\n",
            "    1.9084960e-03  2.7738381e-03]\n",
            "  [ 2.7132845e-03 -2.2334876e-03 -6.0554957e-03 ...  3.5181618e-04\n",
            "    1.1927890e-02  2.2712965e-03]\n",
            "  [ 4.0388266e-03 -3.4512696e-03  1.1636384e-03 ... -1.8924656e-03\n",
            "    9.6672531e-03 -1.1257229e-03]\n",
            "  ...\n",
            "  [ 1.0160389e-02 -1.1949643e-02 -6.6457400e-03 ... -2.3884224e-03\n",
            "   -4.2710286e-03  2.3435790e-04]\n",
            "  [ 1.4835367e-02 -1.3421923e-02 -9.2749000e-03 ... -4.2910771e-03\n",
            "   -6.5809665e-03 -1.9252927e-03]\n",
            "  [ 1.3742216e-02 -1.4986532e-02 -6.8490305e-03 ... -2.4727010e-03\n",
            "   -3.2285200e-03 -2.5030673e-03]]\n",
            "\n",
            " [[ 5.8379346e-03 -3.7997894e-03 -3.1772652e-03 ... -3.3439773e-03\n",
            "   -3.7199245e-03 -1.6107492e-03]\n",
            "  [ 4.5577707e-03 -4.6509434e-03 -6.4611523e-03 ... -1.7492680e-03\n",
            "   -7.2655720e-03  3.1033084e-03]\n",
            "  [ 7.9678297e-03  7.2321307e-04 -1.0280762e-02 ... -4.2583328e-03\n",
            "   -3.1956853e-03  4.9510864e-03]\n",
            "  ...\n",
            "  [ 3.2693106e-03 -6.0217869e-03 -3.6110794e-03 ...  3.2808399e-05\n",
            "    4.3505272e-03 -3.4998269e-03]\n",
            "  [ 7.5031491e-04 -4.7376882e-03 -7.3829060e-04 ... -3.8960106e-03\n",
            "    1.4829484e-03 -6.9540353e-03]\n",
            "  [ 7.2750407e-03 -7.1424297e-03 -4.9600853e-03 ... -5.7064262e-03\n",
            "   -2.0668600e-03 -8.0783479e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 3.9279354e-03 -9.8280283e-04  1.7494310e-03 ... -2.5077108e-03\n",
            "    5.4265291e-04 -2.2723323e-03]\n",
            "  [ 6.5448582e-03  5.0998190e-03 -4.4144150e-03 ... -5.2465135e-03\n",
            "    1.3386854e-03  1.8954317e-03]\n",
            "  [ 5.3799148e-03 -1.8490332e-03 -5.2911979e-03 ... -1.4563107e-03\n",
            "    1.0931638e-02  2.2690059e-03]\n",
            "  ...\n",
            "  [ 4.4607283e-03 -9.8327799e-03 -1.1465723e-02 ...  9.3492987e-03\n",
            "   -1.2878813e-03  2.9014675e-03]\n",
            "  [ 9.9096159e-03 -1.0552479e-02 -1.2502976e-02 ...  4.0673246e-03\n",
            "   -4.4290046e-03  5.2454253e-04]\n",
            "  [ 1.4868053e-02 -6.6112620e-03 -1.0088410e-02 ...  7.3316935e-03\n",
            "   -4.5381044e-03  3.1310157e-04]]\n",
            "\n",
            " [[-9.0268406e-04 -1.1472588e-03 -4.2234631e-03 ...  4.6923268e-04\n",
            "   -5.3005270e-03  5.1387260e-03]\n",
            "  [-1.5670366e-03 -4.1586068e-03 -2.3529010e-03 ... -3.3486064e-03\n",
            "   -6.8121762e-03  3.4151948e-03]\n",
            "  [ 3.0111806e-03 -4.5534996e-03  8.2651142e-04 ... -2.0267139e-03\n",
            "   -1.0840355e-03  1.3991031e-03]\n",
            "  ...\n",
            "  [-4.9510272e-05 -2.1469442e-03 -1.0079309e-02 ...  8.1818560e-03\n",
            "   -8.3505316e-04 -4.4147866e-03]\n",
            "  [-1.0433455e-03  3.2899852e-03 -8.0236122e-03 ...  8.2781436e-03\n",
            "   -5.1892595e-04 -1.1293542e-02]\n",
            "  [-1.7605359e-03  1.1812928e-03 -1.0452148e-02 ...  6.4797890e-03\n",
            "   -3.9564297e-03 -2.4659380e-03]]\n",
            "\n",
            " [[ 1.8306088e-03 -1.3045464e-03  5.5635590e-03 ... -2.3404721e-03\n",
            "    1.5866838e-04 -2.0622087e-03]\n",
            "  [-1.3518841e-03 -6.2572518e-03  1.0434175e-03 ...  2.7028425e-03\n",
            "    4.9380958e-03  1.0252211e-03]\n",
            "  [ 4.2296667e-03 -8.2199322e-03 -2.0181066e-03 ... -1.2273141e-03\n",
            "    9.3111908e-04 -2.6347640e-04]\n",
            "  ...\n",
            "  [-5.6011335e-04 -8.0132962e-04  9.1601806e-03 ... -7.2557232e-03\n",
            "   -1.7750373e-03 -4.2020031e-03]\n",
            "  [-3.7454152e-03 -2.1640788e-04  8.2392115e-03 ... -3.2430417e-03\n",
            "   -2.2724944e-03 -5.7842582e-03]\n",
            "  [-4.2276657e-03 -4.8427037e-03  6.8676947e-03 ... -5.8896160e-03\n",
            "   -4.8809336e-03 -4.7425739e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l54N-ttqnsDo",
        "outputId": "50cb05fe-e076-48ae-94bb-6f312cf530cb"
      },
      "source": [
        "# examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00338668  0.0052238  -0.00563009 ... -0.00315837  0.0019085\n",
            "   0.00277384]\n",
            " [ 0.00457727  0.00124677 -0.00342014 ... -0.00395394  0.00297883\n",
            "   0.00269308]\n",
            " [ 0.00278726 -0.00044395 -0.00549299 ... -0.00222904 -0.00391557\n",
            "   0.0074617 ]\n",
            " ...\n",
            " [ 0.00547342 -0.00872754 -0.0002261  ... -0.00127061 -0.00401402\n",
            "   0.00275173]\n",
            " [ 0.00375004 -0.0061371   0.00103473 ... -0.00370905 -0.01079408\n",
            "   0.0054156 ]\n",
            " [ 0.0063602  -0.00508576  0.00245614 ... -0.00307689 -0.00557941\n",
            "   0.00336218]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaHIUWX_pD8N",
        "outputId": "28a519e7-2708-4102-814f-689797d2f84d"
      },
      "source": [
        "# look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# 65 values represent the probability of each character occuring next"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 3.3866814e-03  5.2238037e-03 -5.6300871e-03 -5.7627680e-05\n",
            " -1.1223005e-02 -1.3835813e-03  1.1171466e-03  1.9499416e-03\n",
            "  6.0982234e-04 -3.6068098e-04  7.5426819e-03  4.2617954e-03\n",
            " -6.6431006e-05 -2.5654838e-03 -2.2227718e-03  2.5029178e-05\n",
            " -6.0074730e-03  1.9740900e-03  6.3079677e-04 -2.9983115e-05\n",
            "  1.5137143e-03  7.7186606e-04 -1.7604164e-03  2.4608870e-03\n",
            " -4.5770584e-03 -2.7333882e-03  1.4805857e-03  1.7160823e-03\n",
            " -4.2330124e-03  1.5195085e-03  2.1826928e-03 -1.1078909e-03\n",
            " -1.2248144e-03  1.1885033e-03 -2.8201253e-03  3.1072595e-03\n",
            "  2.6906584e-03  2.4038535e-03 -3.9259810e-04  8.2979759e-04\n",
            " -1.8729249e-03  1.9882252e-03 -4.9314266e-03  5.6119310e-04\n",
            " -1.9450617e-03 -1.3574122e-03  3.4473634e-03 -9.3007158e-04\n",
            " -2.3548598e-03 -1.9033084e-04 -1.2099512e-03 -5.6040930e-03\n",
            " -2.9555117e-03  1.2872345e-04  3.7422122e-03  2.8477637e-03\n",
            " -2.4774706e-03 -2.9872719e-03 -8.9110481e-04 -4.5457734e-03\n",
            "  4.1934825e-04  4.4707851e-03 -3.1583738e-03  1.9084960e-03\n",
            "  2.7738381e-03], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yo7wleropP0j",
        "outputId": "967c72eb-b3bc-420c-f849-694bda726e06"
      },
      "source": [
        "# to determine the predicted character, it is necessary to sample the output distribution (pick a value based on probability)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples = 1)\n",
        "\n",
        "# reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars   # this is what the model predicted for training sequence 1"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"aE iYVtf3enKARir-BsqUpGub\\nncZn?qBFvI&dS$';;trFV!!CK ,n?LB&S3N&;-PP?JnsMqs;;'pZJdItUuR!v.$ci&;FUrcvUI\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HMDwpYvqpdm"
      },
      "source": [
        "# create a loss function that can compare that output to the expected output and give some numeric value representing how close the two were\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXAPlgbFr5E4"
      },
      "source": [
        "## **Compiling the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxkpCH2xr_Px"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = loss)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R11cbn6vsDY-"
      },
      "source": [
        "## **Creating Checkpoints**\n",
        "This will allow us to load the model from a checkpoint and continue training it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEmIg1LosLy3"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only = True\n",
        ")"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVDqEE8seoN"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv3ka33ssf8u",
        "outputId": "7cb647b9-326d-410c-de0d-16e7dcfca886"
      },
      "source": [
        "history = model.fit(data, epochs = 40, callbacks = [checkpoint_callback])"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 13s 55ms/step - loss: 3.0313\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9428\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.6551\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.5044\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.4176\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.3618\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3124\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2695\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2312\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.1926\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.1546\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.1154\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0770\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.0384\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.9958\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.9530\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.9113\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8721\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8322\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7946\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.7585\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.7266\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.6932\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.6680\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.6419\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.6202\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5966\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5776\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5609\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.5477\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5314\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5199\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5091\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.4972\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.4882\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.4809\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.4716\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.4662\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.4582\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.4562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnb_bylcss2f"
      },
      "source": [
        "## **Loading the Model**\n",
        "Rebuild the model from a checkpoing using a batch_size of 1 so that we can feed one piece of text to the model and have it make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL_VmX0ys3vp"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size = 1)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU5N1Cn7s_8k"
      },
      "source": [
        "# find the latest checkpoint that stores the models weights\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVA3vqdbuAGx"
      },
      "source": [
        "## **Generating Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLgZ7Wg-uFgu"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text\n",
        "  # Higher temperatures results in more surprising text\n",
        "  # Experiment to find the best setting\n",
        "  temperature = 1.0\n",
        "\n",
        "  # batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1, 0].numpy()\n",
        "\n",
        "    # we pass the predicted character as the next input to the model along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGY98ucVzepR",
        "outputId": "077ceb81-be71-4fed-c8bb-5d1d451ac6e7"
      },
      "source": [
        "inp = input('Type a starting string: ')\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: romeo\n",
            "romeous to remember:\n",
            "If I be talk'd with a swaingul-be-trick\n",
            "And titherselves to tell my thind shall go to you,\n",
            "Unless you will perform it to my lord or end.\n",
            "Take him upon you and home for thine!\n",
            "Let no man holds up Langar over graced, my lord\n",
            "Before I board the sad stamp of love in ill!\n",
            "And arm dear valour; let him call back and fly.\n",
            "\n",
            "BAPTISTA:\n",
            "Gentlemen, content thms?\n",
            "\n",
            "First Servant:\n",
            "Why, sir, God forbid Cornord,\n",
            "Is this the heavy mile and earth against my cansward, sir, in any way from him that was born:\n",
            "Were it betide their cames that still intiments\n",
            "And in his ward up so griev, and hapling thee--\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "O, mighty Frethy hope, with Richmond Angelo\n",
            "And in Bohemia told shifts, thereon exchance,\n",
            "We leann you to your testingely to encounter mine.\n",
            "\n",
            "EXETER:\n",
            "Away, away!\n",
            "\n",
            "SEBASTIAN:\n",
            "'Sca\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}